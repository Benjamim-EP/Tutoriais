{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 align = 'center'> Atribuição e Atualização de valores dos Pesos e Neuronios </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>\n",
    "   Os valores das sinapses e dos neuronios são modificados constantemente durante o processo de treinamento, e para isto ser feito o algoritmo deve ser capaz de acessa-los constantemente, isto é feito por  metodos chamados \"forward\" ou \"Backpropagation\".\n",
    "</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Antes de tudo vamos a uma declaração mais formal das nossas variaveis que serão usadas</p>\n",
    "<p>Isto é importante pois trataremos varios artigos cientificos recentes em que o tratamento formal matemático deve estar no sangue</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align = 'center'> Variables </h3>\n",
    "\n",
    "|Code Symbol | Math Symbol | Definition | Dimensions\n",
    "| :-: | :-: | :-: | :-: |\n",
    "|X|$$X$$|Input Data, each row in an example| (numExamples, inputLayerSize)|\n",
    "|y |$$y$$|target data|(numExamples, outputLayerSize)|\n",
    "|W1 | $$W^{(1)}$$ | Layer 1 weights | (inputLayerSize, hiddenLayerSize) |\n",
    "|W2 | $$W^{(2)}$$ | Layer 2 weights | (hiddenLayerSize, outputLayerSize) |\n",
    "|z2 | $$z^{(2)}$$ | Layer 2 activation | (numExamples, hiddenLayerSize) |\n",
    "|a2 | $$a^{(2)}$$ | Layer 2 activity | (numExamples, hiddenLayerSize) |\n",
    "|z3 | $$z^{(3)}$$ | Layer 3 activation | (numExamples, outputLayerSize) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Exemplo de rede Neural com nomenclaturas </strong>\n",
    "![title](imgs/NN3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4) (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# Utilizaremos o exemplo do primeiro arquivo\n",
    "X = np.array(([10,20,30,40], [50,10,55,30], [10,20,45,90]), dtype=float)\n",
    "y = np.array(([22000], [45000], [60000]), dtype=float)\n",
    "# Normalizando-os\n",
    "X = X/np.amax(X, axis=0) \n",
    "y = y/100000\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "#    (3, 4) -> X = Entrada, contem 3 exemplos, cada um com um vetor de dados de entrada da Rede Neural (4)\n",
    "#           ->     ex: [10,20,30,40] = 1 exemplo, os 4 valores que estão no vetor serão o Input (X) da rede neural \n",
    "#    (3, 1) -> y = Target data, saida da rede neural, 3 exemplos e 1 valor para cada exemplo\n",
    "#                = Cada um se refere a um dos exemplos do input (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>\n",
    "  Primeira Estrutura da nossa Rede Neural\n",
    "</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neste exemplo utilizaremos o metodo forward propagation (propagação para frente)de movimentação dos dados \n",
    "# para a atualização \n",
    "# das sinapses e neuronios\n",
    "# Este metodo será explicado e abordado a seguir\n",
    "class Neural_Network():\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.TamanhoCamadaEntrada = 4   # (InputLayerSize)Quantos Neuronios (parametros) tem a camada de entrada \n",
    "        self.TamanhoCamadaOculta  = 3   # (HiddenLayerSize)Quantos Neuronios (parametros) tem a camada de Oculta\n",
    "        self.TamanhoCamadaSaida   = 1   # (OutputLayerSize)Quantos Neuronios (parametros) tem a camada de Saida\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propagate inputs though network\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Importante para Fixação:</h2>\n",
    "\n",
    "Cada neuronio de uma camada oculta é resultado do somatorio da  multiplicação de cada valor de entrada pelo peso que chegue\n",
    "a este neuronio da camada oculta, no entanto este resultado (que é uma multiplicação de matrizes) precisa ser \"convertido\",\n",
    "e para isto que se utilizam as funções de ativação \n",
    "\n",
    "\n",
    "A imagem a seguir representa muito bem o que esta sendo dito\n",
    "![title](imgs/NN5.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "w's = valores dos pesos (Neuronios weigths)\n",
    "x's = valores das entradas (Neuronios inputs)\n",
    "b's = bias (abordado posteriormente)\n",
    "a's = outputs (valores finais apos passarem pela função de ativação)\n",
    "\n",
    "Observe que a resposta da multiplicação da matriz sempre tera a dimensão da camada seguinte (um vetor com n posições)e seus valores ainda terão que passar por uma função de ativação, para então serem \"oficialmente\" os valores dos neuronios dessa\n",
    "camada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada valor de entrada, ou elemento na matriz X, precisa ser multiplicado por um peso correspondente e, em seguida,\n",
    "adicionado junto com todos os outros resultados para cada neurônio\n",
    "<h1>1°)</h1><h3>Multiplicar as Entradas(X) pelos pesos(W)</h3>\n",
    "\n",
    "\n",
    "$z^{(2)} = XW^{(1)}$        \n",
    "\n",
    "$z^{(2)}$ =  a atividade da segunda camada(a oculta), representa a soma ponderada,  cada entrada em z é uma soma de entradas                      ponderadas para cada neurônio oculto.Dimensão 3x3 -> uma linha para cada exemplo e uma coluna para cada unidade oculta.\n",
    "\n",
    "$XW^{(1)}$  = Cada valor de entrada, ou elemento na matriz X, é multiplicado por um peso correspondente    \n",
    "\n",
    "<h1>2°)</h1><h3>Aplicar a Função de Ativação</h3>\n",
    "A função de ativação utilizada será a sigmoid, a sua formula e seu grafico é o que se segue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/NN4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação da função\n",
    "def sigmoid(z):\n",
    "    #Aplicando a função sigmoid\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a^{(2)} = f(z^{(2)}) \\tag{2}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funçãode ativação\n",
    "$$\n",
    "f \n",
    "$$\n",
    "Atividade da 2° camada\n",
    "$$\n",
    "a^{(2)}\n",
    "$$\n",
    "Atividade da 2° camada (Antes da função de ativação)\n",
    "$$\n",
    "z^{(2)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Fazendo isto consecutivamente até a camada de saida</h5>\n",
    "\n",
    "$$\n",
    "z^{(3)} = a^{(2)}W^{(2)} \\tag{3}\\\\\n",
    "$$\n",
    "$$\n",
    "\\hat{y} = f(z^{(3)}) \\tag{4}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Implementação do algoritmo das formulas</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Hiperparametros\n",
    "        self.inputLayerSize = 2   # Camada de entrada (2 valores de entrada)\n",
    "        self.outputLayerSize = 1  # Camada de Saida   (1 valor de saida)\n",
    "        self.hiddenLayerSize = 3  # Camada oculta     (3 valores)\n",
    "        \n",
    "        #Pesos (sinapses)\n",
    "        # Sinapses que ligam a camada de entrada a camada oculta\n",
    "        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        # Sinapses que ligam a camada oculta a camada de saida\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "    \n",
    "    # OBS : A função .dot faz o produto escalar de duas matrizes, processo muito mais rapido que a multiplicação tradicional\n",
    "    #       de matrizes\n",
    "    def forward(self, X):\n",
    "        #Propagar inputs na rede neural\n",
    "        self.z2 = np.dot(X, self.W1)       # Multiplicação dos valores de entrada pelos pesos \n",
    "                                           # Estes vão ser os valores da camada camada oculta antes da função de ativação\n",
    "        self.a2 = self.sigmoid(self.z2)    # Aplicação da função de ativação na matriz resultante (z2)\n",
    "                                           # Camada oculta apos a função de ativação\n",
    "        self.z3 = np.dot(self.a2, self.W2) # Multiplicação da camada oculta pelos pesos que a ligam a camada de saida\n",
    "        yHat = self.sigmoid(self.z3)       # Camada de saida  (resultado) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
